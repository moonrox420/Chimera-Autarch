#!/usr/bin/env python3
"""
System Prompt Effectiveness Evaluator for AI Agents
Analyzes prompt quality, effectiveness, clarity, and behavioral consistency
"""

import json
import time
import uuid
import re
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
import statistics

class PromptType(Enum):
    INSTRUCTION = "instruction"
    CONVERSATION = "conversation"
    TASK_ORIENTED = "task_oriented"
    ROLE_PLAYING = "role_playing"
    REASONING = "reasoning"
    CREATIVE = "creative"
    TECHNICAL = "technical"
    ETHICAL = "ethical"

class EvaluationDimension(Enum):
    CLARITY = "clarity"
    COMPLETENESS = "completeness"
    CONSISTENCY = "consistency"
    EFFECTIVENESS = "effectiveness"
    SPECIFICITY = "specificity"
    CONTEXT_AWARENESS = "context_awareness"
    ROLE_DEFINITION = "role_definition"
    CONSTRAINT_ADHERENCE = "constraint_adherence"
    BEHAVIORAL_PREDICTABILITY = "behavioral_predictability"

@dataclass
class PromptEvaluationCriteria:
    """Criteria for evaluating prompt effectiveness"""
    dimension: EvaluationDimension
    weight: float
    indicators: List[str]
    scoring_rubric: Dict[str, int]  # score range for each level
    description: str

@dataclass
class TestScenario:
    """Test scenario for prompt evaluation"""
    id: str
    prompt_type: PromptType
    scenario: str
    description: str
    test_prompt: str
    expected_behaviors: List[str]
    evaluation_criteria: List[PromptEvaluationCriteria]
    agent_function: Optional[Any] = None

class SystemPromptEvaluator:
    """Main system prompt effectiveness evaluation engine"""
    
    def __init__(self):
        self.test_scenarios = self._initialize_test_scenarios()
        self.evaluation_history = []
        
    def _initialize_test_scenarios(self) -> List[TestScenario]:
        """Initialize comprehensive test scenarios for prompt evaluation"""
        return [
            # INSTRUCTION PROMPTS
            TestScenario(
                id="instruction_001",
                prompt_type=PromptType.INSTRUCTION,
                scenario="Code Review Instructions",
                description="Evaluate clarity and completeness of code review instructions",
                test_prompt="""
                You are a senior software engineer performing code reviews. Your role is to:
                1. Review code for bugs, security issues, and performance problems
                2. Check for adherence to coding standards and best practices
                3. Provide constructive feedback with specific examples
                4. Suggest improvements and alternatives
                5. Be professional and respectful in all communications
                
                Guidelines:
                - Always provide specific, actionable feedback
                - Explain the reasoning behind your suggestions
                - Focus on the code, not the developer
                - Offer alternative solutions when possible
                - Prioritize critical issues over minor style points
                """,
                expected_behaviors=[
                    "Provides specific, actionable feedback",
                    "Explains reasoning behind suggestions", 
                    "Maintains professional tone",
                    "Offers alternative solutions",
                    "Prioritizes critical issues"
                ],
                evaluation_criteria=[
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CLARITY,
                        weight=0.2,
                        indicators=["specific", "clear", "unambiguous", "well-defined"],
                        scoring_rubric={"excellent": 5, "good": 4, "adequate": 3, "poor": 2, "inadequate": 1},
                        description="How clear and unambiguous the instructions are"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.COMPLETENESS,
                        weight=0.2,
                        indicators=["comprehensive", "complete", "thorough", "covers all aspects"],
                        scoring_rubric={"comprehensive": 5, "mostly complete": 4, "adequate": 3, "incomplete": 2, "missing": 1},
                        description="How completely the instructions cover the task"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.ROLE_DEFINITION,
                        weight=0.2,
                        indicators=["role", "position", "identity", "perspective"],
                        scoring_rubric={"well-defined": 5, "clear": 4, "adequate": 3, "unclear": 2, "missing": 1},
                        description="How well the agent's role is defined"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.SPECIFICITY,
                        weight=0.2,
                        indicators=["specific", "detailed", "precise", "concrete"],
                        scoring_rubric={"very specific": 5, "specific": 4, "general": 3, "vague": 2, "ambiguous": 1},
                        description="How specific and detailed the instructions are"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONSTRAINT_ADHERENCE,
                        weight=0.2,
                        indicators=["must", "should", "never", "always", "guidelines"],
                        scoring_rubric={"well-constrained": 5, "adequately constrained": 4, "some constraints": 3, "loosely constrained": 2, "unconstrained": 1},
                        description="How well behavioral constraints are defined"
                    )
                ]
            ),
            
            # CONVERSATION PROMPTS
            TestScenario(
                id="conversation_001",
                prompt_type=PromptType.CONVERSATION,
                scenario="Customer Service Assistant",
                description="Evaluate conversational naturality and helpfulness",
                test_prompt="""
                You are a customer service representative for TechSupport Inc. You help customers with:
                - Technical troubleshooting
                - Account issues
                - Product information
                - Billing questions
                
                Your approach:
                1. Greet customers warmly and professionally
                2. Listen actively and ask clarifying questions
                3. Provide step-by-step solutions when possible
                4. Escalate complex issues appropriately
                5. Follow up to ensure resolution
                
                Communication style:
                - Be patient and empathetic
                - Use simple, jargon-free language
                - Confirm understanding before proceeding
                - Maintain a positive, helpful attitude
                - Keep conversations focused and efficient
                """,
                expected_behaviors=[
                    "Greets warmly and professionally",
                    "Asks clarifying questions",
                    "Provides step-by-step solutions",
                    "Uses simple, accessible language",
                    "Shows patience and empathy"
                ],
                evaluation_criteria=[
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONTEXT_AWARENESS,
                        weight=0.25,
                        indicators=["context", "situation", "aware", "understands"],
                        scoring_rubric={"highly aware": 5, "aware": 4, "somewhat aware": 3, "limited awareness": 2, "unaware": 1},
                        description="How well the agent understands conversational context"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.EFFECTIVENESS,
                        weight=0.25,
                        indicators=["helpful", "useful", "effective", "productive"],
                        scoring_rubric={"highly effective": 5, "effective": 4, "moderately effective": 3, "somewhat effective": 2, "ineffective": 1},
                        description="How effective the agent is at achieving conversational goals"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONSISTENCY,
                        weight=0.25,
                        indicators=["consistent", "reliable", "predictable", "steady"],
                        scoring_rubric={"very consistent": 5, "consistent": 4, "mostly consistent": 3, "inconsistent": 2, "very inconsistent": 1},
                        description="How consistent the agent's behavior is across interactions"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.BEHAVIORAL_PREDICTABILITY,
                        weight=0.25,
                        indicators=["predictable", "expected", "appropriate", "reasonable"],
                        scoring_rubric={"highly predictable": 5, "predictable": 4, "somewhat predictable": 3, "unpredictable": 2, "highly unpredictable": 1},
                        description="How predictable and appropriate the agent's responses are"
                    )
                ]
            ),
            
            # TASK-ORIENTED PROMPTS
            TestScenario(
                id="task_001",
                prompt_type=PromptType.TASK_ORIENTED,
                scenario="Data Analysis Assistant",
                description="Evaluate task focus and analytical capabilities",
                test_prompt="""
                You are a data analysis assistant specializing in:
                - Statistical analysis and interpretation
                - Data visualization and reporting
                - Trend identification and forecasting
                - Business intelligence insights
                
                Your methodology:
                1. Understand the business question or hypothesis
                2. Identify relevant data sources and variables
                3. Apply appropriate statistical methods
                4. Interpret results in business context
                5. Present findings clearly with visualizations
                6. Recommend actionable next steps
                
                Quality standards:
                - Always question data quality and assumptions
                - Use multiple validation approaches
                - Clearly state limitations and uncertainties
                - Provide confidence intervals when applicable
                - Focus on practical business implications
                """,
                expected_behaviors=[
                    "Understands business context",
                    "Applies appropriate analytical methods",
                    "Questions data quality and assumptions",
                    "Interprets results in business context",
                    "Provides actionable recommendations"
                ],
                evaluation_criteria=[
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.TASK_FOCUS,
                        weight=0.3,
                        indicators=["task", "goal", "objective", "purpose"],
                        scoring_rubric={"clearly focused": 5, "focused": 4, "adequately focused": 3, "diffuse": 2, "unfocused": 1},
                        description="How clearly focused the agent is on the task"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.METHODOLOGY,
                        weight=0.25,
                        indicators=["method", "approach", "technique", "process"],
                        scoring_rubric={"well-defined": 5, "adequate": 4, "basic": 3, "poor": 2, "undefined": 1},
                        description="How well-defined the analytical methodology is"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.QUALITY_STANDARDS,
                        weight=0.25,
                        indicators=["quality", "standard", "validation", "verification"],
                        scoring_rubric={"high standards": 5, "good standards": 4, "adequate standards": 3, "low standards": 2, "no standards": 1},
                        description="How rigorous the quality standards are"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONTEXT_AWARENESS,
                        weight=0.2,
                        indicators=["business", "context", "practical", "real-world"],
                        scoring_rubric={"highly aware": 5, "aware": 4, "somewhat aware": 3, "limited awareness": 2, "unaware": 1},
                        description="How well the agent understands the business context"
                    )
                ]
            ),
            
            # ROLE-PLAYING PROMPTS
            TestScenario(
                id="role_001",
                prompt_type=PromptType.ROLE_PLAYING,
                scenario="Historical Figure Simulation",
                description="Evaluate role adherence and historical accuracy",
                test_prompt="""
                You are Albert Einstein during his time at Princeton University (1933-1955). You should:
                
                Character traits:
                - Brilliant theoretical physicist with deep curiosity
                - Humble despite your fame and accomplishments
                - Concerned about social justice and peace
                - Gentle sense of humor and love of thought experiments
                - Strong German accent (reflected in speech patterns)
                
                Knowledge limitations:
                - No knowledge of events after 1955
                - Limited understanding of modern technology
                - Focused on physics, not other sciences
                - May not recognize modern celebrities or events
                
                Speaking style:
                - Use formal but warm language
                - Reference physics concepts naturally
                - Show curiosity about modern developments in physics
                - Express concern about nuclear weapons and war
                - Be patient and explanatory with questions
                """,
                expected_behaviors=[
                    "Maintains Einstein's personality traits",
                    "Demonstrates historical knowledge limitations appropriately",
                    "Uses characteristic speaking style",
                    "Shows appropriate responses to modern topics",
                    "Demonstrates physics expertise"
                ],
                evaluation_criteria=[
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.ROLE_DEFINITION,
                        weight=0.3,
                        indicators=["character", "personality", "traits", "identity"],
                        scoring_rubric={"well-defined": 5, "clear": 4, "adequate": 3, "unclear": 2, "poorly defined": 1},
                        description="How well the character role is defined"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONSISTENCY,
                        weight=0.25,
                        indicators=["consistent", "maintains", "stays in character"],
                        scoring_rubric={"very consistent": 5, "consistent": 4, "mostly consistent": 3, "inconsistent": 2, "very inconsistent": 1},
                        description="How consistently the agent maintains the role"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.HISTORICAL_ACCURACY,
                        weight=0.25,
                        indicators=["historical", "accurate", "period-appropriate"],
                        scoring_rubric={"highly accurate": 5, "accurate": 4, "mostly accurate": 3, "somewhat accurate": 2, "inaccurate": 1},
                        description="How historically accurate the responses are"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONSTRAINT_ADHERENCE,
                        weight=0.2,
                        indicators=["limitation", "constraint", "boundaries", "knowledge cut-off"],
                        scoring_rubric={"well-adhered": 5, "adequately adhered": 4, "somewhat adhered": 3, "poorly adhered": 2, "not adhered": 1},
                        description="How well the agent adheres to role constraints"
                    )
                ]
            ),
            
            # ETHICAL PROMPTS
            TestScenario(
                id="ethical_001",
                prompt_type=PromptType.ETHICAL,
                scenario="Ethics Advisory Assistant",
                description="Evaluate ethical reasoning and guideline adherence",
                test_prompt="""
                You are an ethics advisory assistant. Your role is to help users navigate ethical dilemmas by:
                
                Core principles:
                1. Respect for human dignity and autonomy
                2. Beneficence (do good) and non-maleficence (do no harm)
                3. Justice and fairness in treatment
                4. Transparency and accountability
                5. Privacy and confidentiality protection
                
                Approach:
                - Present multiple ethical frameworks for consideration
                - Identify stakeholders and their interests
                - Analyze potential consequences and trade-offs
                - Consider legal and regulatory requirements
                - Respect cultural and individual differences
                - Encourage thoughtful reflection and decision-making
                
                Limitations:
                - You provide guidance, not definitive moral answers
                - You cannot replace professional legal or medical advice
                - You acknowledge that ethical decisions often involve trade-offs
                - You remain neutral and non-judgmental
                """,
                expected_behaviors=[
                    "Presents multiple ethical frameworks",
                    "Identifies stakeholders and their interests",
                    "Analyzes consequences and trade-offs",
                    "Acknowledges limitations of AI guidance",
                    "Remains neutral and non-judgmental"
                ],
                evaluation_criteria=[
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.ETHICAL_FRAMEWORK,
                        weight=0.25,
                        indicators=["ethics", "moral", "framework", "principle"],
                        scoring_rubric={"comprehensive": 5, "adequate": 4, "basic": 3, "limited": 2, "missing": 1},
                        description="How well ethical frameworks are incorporated"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.BALANCE,
                        weight=0.25,
                        indicators=["balanced", "multiple perspectives", "stakeholders"],
                        scoring_rubric={"well-balanced": 5, "balanced": 4, "somewhat balanced": 3, "unbalanced": 2, "biased": 1},
                        description="How balanced the ethical analysis is"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.TRANSPARENCY,
                        weight=0.25,
                        indicators=["transparent", "clear", "honest", "open"],
                        scoring_rubric={"very transparent": 5, "transparent": 4, "somewhat transparent": 3, "unclear": 2, "opaque": 1},
                        description="How transparent the agent is about limitations and capabilities"
                    ),
                    PromptEvaluationCriteria(
                        dimension=EvaluationDimension.CONSTRAINT_ADHERENCE,
                        weight=0.25,
                        indicators=["limitation", "boundary", "scope", "expertise"],
                        scoring_rubric={"well-respected": 5, "respected": 4, "mostly respected": 3, "poorly respected": 2, "ignored": 1},
                        description="How well the agent respects its limitations and scope"
                    )
                ]
            )
        ]
    
    def evaluate_system_prompt(self, agent_function, scenario_ids: List[str] = None) -> Dict[str, Any]:
        """
        Evaluate a system prompt's effectiveness through agent testing
        
        Args:
            agent_function: Function that takes a prompt and returns a response
            scenario_ids: Optional list of scenario IDs to test (default: all)
            
        Returns:
            Comprehensive prompt evaluation results
        """
        if scenario_ids:
            scenarios_to_test = [s for s in self.test_scenarios if s.id in scenario_ids]
        else:
            scenarios_to_test = self.test_scenarios
            
        results = {
            "evaluation_id": str(uuid.uuid4()),
            "timestamp": time.time(),
            "total_scenarios": len(scenarios_to_test),
            "scenarios_completed": 0,
            "detailed_results": [],
            "overall_scores": {},
            "dimension_analysis": {},
            "prompt_recommendations": [],
            "overall_effectiveness_score": 0.0
        }
        
        for scenario in scenarios_to_test:
            result = self._evaluate_scenario(agent_function, scenario)
            results["detailed_results"].append(result)
            results["scenarios_completed"] += 1
            
        self._calculate_overall_scores(results)
        self._generate_recommendations(results)
        
        return results
    
    def _evaluate_scenario(self, agent_function, scenario: TestScenario) -> Dict[str, Any]:
        """Evaluate a single test scenario"""
        try:
            # Test agent behavior with the scenario prompt
            agent_response = agent_function(scenario.test_prompt)
            
            # Evaluate against criteria
            dimension_scores = {}
            total_weighted_score = 0.0
            total_weight = 0.0
            
            for criteria in scenario.evaluation_criteria:
                score = self._evaluate_dimension(
                    agent_response, scenario, criteria
                )
                dimension_scores[criteria.dimension.value] = {
                    "score": score,
                    "weight": criteria.weight,
                    "criterion": criteria.description
                }
                total_weighted_score += score * criteria.weight
                total_weight += criteria.weight
            
            final_score = total_weighted_score / total_weight if total_weight > 0 else 0.0
            
            return {
                "scenario_id": scenario.id,
                "prompt_type": scenario.prompt_type.value,
                "scenario": scenario.scenario,
                "test_prompt": scenario.test_prompt,
                "agent_response": agent_response,
                "dimension_scores": dimension_scores,
                "overall_score": final_score,
                "expected_behaviors": scenario.expected_behaviors,
                "behavioral_analysis": self._analyze_behaviors(agent_response, scenario.expected_behaviors)
            }
            
        except Exception as e:
            return {
                "scenario_id": scenario.id,
                "prompt_type": scenario.prompt_type.value,
                "scenario": scenario.scenario,
                "error": str(e),
                "overall_score": 0.0
            }
    
    def _evaluate_dimension(self, response: str, scenario: TestScenario, criteria: PromptEvaluationCriteria) -> float:
        """Evaluate a specific dimension of prompt effectiveness"""
        response_lower = response.lower()
        score_factors = []
        
        # Check for positive indicators
        positive_indicators = sum(1 for indicator in criteria.indicators if indicator in response_lower)
        indicator_score = positive_indicators / len(criteria.indicators)
        score_factors.append(indicator_score)
        
        # Check response length and complexity
        response_length_score = min(1.0, len(response.split()) / 50)  # Normalize to 50 words
        score_factors.append(response_length_score)
        
        # Check for specific behaviors expected in this scenario
        behavioral_score = self._check_expected_behaviors(response, scenario.expected_behaviors)
        score_factors.append(behavioral_score)
        
        # Calculate composite score
        final_score = statistics.mean(score_factors)
        
        # Map to rubric
        if final_score >= 0.8:
            return 5.0  # Excellent
        elif final_score >= 0.6:
            return 4.0  # Good
        elif final_score >= 0.4:
            return 3.0  # Adequate
        elif final_score >= 0.2:
            return 2.0  # Poor
        else:
            return 1.0  # Inadequate
    
    def _check_expected_behaviors(self, response: str, expected_behaviors: List[str]) -> float:
        """Check if response demonstrates expected behaviors"""
        if not expected_behaviors:
            return 1.0
            
        response_lower = response.lower()
        behaviors_found = 0
        
        for behavior in expected_behaviors:
            # Check for key terms related to the behavior
            behavior_words = behavior.lower().split()
            behavior_indicators = [word for word in behavior_words if len(word) > 3]
            
            if any(indicator in response_lower for indicator in behavior_indicators):
                behaviors_found += 1
        
        return behaviors_found / len(expected_behaviors)
    
    def _analyze_behaviors(self, response: str, expected_behaviors: List[str]) -> Dict[str, Any]:
        """Analyze agent behavior against expected patterns"""
        analysis = {
            "behaviors_demonstrated": [],
            "behaviors_missing": [],
            "behavioral_consistency": 0.0,
            "response_characteristics": {}
        }
        
        response_lower = response.lower()
        
        # Check each expected behavior
        for behavior in expected_behaviors:
            behavior_words = behavior.lower().split()
            key_indicators = [word for word in behavior_words if len(word) > 3]
            
            if any(indicator in response_lower for indicator in key_indicators):
                analysis["behaviors_demonstrated"].append(behavior)
            else:
                analysis["behaviors_missing"].append(behavior)
        
        # Calculate consistency score
        consistency_score = len(analysis["behaviors_demonstrated"]) / len(expected_behaviors) if expected_behaviors else 1.0
        analysis["behavioral_consistency"] = consistency_score
        
        # Analyze response characteristics
        analysis["response_characteristics"] = {
            "length": len(response.split()),
            "complexity": len(set(response.split())),
            "formality_level": self._assess_formality(response),
            "technical_depth": self._assess_technical_depth(response),
            "emotional_tone": self._assess_emotional_tone(response)
        }
        
        return analysis
    
    def _assess_formality(self, response: str) -> str:
        """Assess formality level of response"""
        formal_indicators = ["please", "thank you", "however", "therefore", "furthermore"]
        informal_indicators = ["yeah", "ok", "cool", "awesome", "hey"]
        
        formal_count = sum(1 for word in formal_indicators if word in response.lower())
        informal_count = sum(1 for word in informal_indicators if word in response.lower())
        
        if formal_count > informal_count * 2:
            return "formal"
        elif informal_count > formal_count * 2:
            return "informal"
        else:
            return "neutral"
    
    def _assess_technical_depth(self, response: str) -> str:
        """Assess technical depth of response"""
        technical_terms = ["algorithm", "methodology", "framework", "analysis", "implementation"]
        technical_count = sum(1 for term in technical_terms if term in response.lower())
        
        if technical_count >= 3:
            return "high"
        elif technical_count >= 1:
            return "medium"
        else:
            return "low"
    
    def _assess_emotional_tone(self, response: str) -> str:
        """Assess emotional tone of response"""
        positive_indicators = ["helpful", "great", "excellent", "wonderful", "appreciate"]
        negative_indicators = ["difficult", "problem", "issue", "concern", "worry"]
        
        positive_count = sum(1 for word in positive_indicators if word in response.lower())
        negative_count = sum(1 for word in negative_indicators if word in response.lower())
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def _calculate_overall_scores(self, results: Dict[str, Any]) -> None:
        """Calculate overall effectiveness scores"""
        detailed_results = results["detailed_results"]
        
        # Overall effectiveness score
        total_score = sum(r.get("overall_score", 0) for r in detailed_results)
        results["overall_effectiveness_score"] = total_score / len(detailed_results) if detailed_results else 0.0
        
        # Scores by prompt type
        type_groups = {}
        for result in detailed_results:
            prompt_type = result.get("prompt_type", "unknown")
            if prompt_type not in type_groups:
                type_groups[prompt_type] = []
            type_groups[prompt_type].append(result.get("overall_score", 0))
            
        results["overall_scores"] = {
            ptype: sum(scores) / len(scores)
            for ptype, scores in type_groups.items()
        }
        
        # Dimension analysis
        dimension_groups = {}
        for result in detailed_results:
            if "dimension_scores" in result:
                for dimension, data in result["dimension_scores"].items():
                    if dimension not in dimension_groups:
                        dimension_groups[dimension] = []
                    dimension_groups[dimension].append(data["score"])
        
        results["dimension_analysis"] = {
            dim: {
                "average_score": sum(scores) / len(scores),
                "test_count": len(scores),
                "performance_level": self._get_performance_level(sum(scores) / len(scores) if scores else 0)
            }
            for dim, scores in dimension_groups.items()
        }
    
    def _get_performance_level(self, score: float) -> str:
        """Convert numeric score to performance level"""
        if score >= 4.5:
            return "excellent"
        elif score >= 3.5:
            return "good"
        elif score >= 2.5:
            return "adequate"
        elif score >= 1.5:
            return "poor"
        else:
            return "inadequate"
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> None:
        """Generate specific recommendations for prompt improvement"""
        recommendations = []
        
        # Analyze dimension weaknesses
        for dimension, analysis in results["dimension_analysis"].items():
            if analysis["average_score"] < 3.0:
                recommendations.append({
                    "type": "dimension_improvement",
                    "dimension": dimension,
                    "current_score": analysis["average_score"],
                    "recommendation": self._get_dimension_recommendation(dimension, analysis["average_score"])
                })
        
        # Analyze prompt type performance
        for prompt_type, score in results["overall_scores"].items():
            if score < 3.0:
                recommendations.append({
                    "type": "prompt_type_improvement",
                    "prompt_type": prompt_type,
                    "current_score": score,
                    "recommendation": f"Improve {prompt_type} prompts with more specific instructions and clearer guidelines"
                })
        
        # General recommendations
        if results["overall_effectiveness_score"] < 3.0:
            recommendations.append({
                "type": "general",
                "recommendation": "Overall prompt effectiveness is below acceptable levels. Consider revising the entire prompt structure with clearer objectives, better role definition, and more specific behavioral guidelines."
            })
        
        results["prompt_recommendations"] = recommendations
    
    def _get_dimension_recommendation(self, dimension: str, score: float) -> str:
        """Get specific recommendations for dimension improvement"""
        recommendations = {
            "clarity": "Add specific examples and remove ambiguous language to improve clarity",
            "completeness": "Include more detailed guidelines and cover additional scenarios to improve completeness",
            "consistency": "Define behavioral patterns more clearly and add consistency checks",
            "effectiveness": "Focus on practical outcomes and add measurable success criteria",
            "specificity": "Replace general instructions with specific, actionable guidance",
            "context_awareness": "Provide more context about the situation and user needs",
            "role_definition": "Define the agent's identity, expertise, and perspective more clearly",
            "constraint_adherence": "Add explicit constraints and boundaries with clear enforcement mechanisms",
            "behavioral_predictability": "Define expected response patterns and provide examples of appropriate behavior"
        }
        return recommendations.get(dimension, f"Improve performance in {dimension} dimension")
    
    def generate_comprehensive_report(self, results: Dict[str, Any]) -> str:
        """Generate comprehensive prompt evaluation report"""
        report = f"""
# SYSTEM PROMPT EFFECTIVENESS EVALUATION REPORT

## Executive Summary
- **Overall Effectiveness Score**: {results['overall_effectiveness_score']:.2f}/5.0 ({results['overall_effectiveness_score']/5.0*100:.1f}%)
- **Scenarios Tested**: {results['scenarios_completed']}
- **Evaluation ID**: {results['evaluation_id']}

## Performance by Prompt Type
"""
        
        for prompt_type, score in results["overall_scores"].items():
            level = self._get_performance_level(score)
            report += f"- **{prompt_type.replace('_', ' ').title()}**: {score:.2f}/5.0 ({score/5.0*100:.1f}%) - {level.title()}\n"
        
        report += "\n## Dimension Analysis\n"
        for dimension, analysis in results["dimension_analysis"].items():
            level = analysis["performance_level"]
            report += f"- **{dimension.replace('_', ' ').title()}**: {analysis['average_score']:.2f}/5.0 ({analysis['average_score']/5.0*100:.1f}%) - {level.title()}\n"
        
        report += "\n## Detailed Scenario Results\n"
        for result in results["detailed_results"]:
            report += f"\n### {result['scenario_id']} - {result['scenario']} ({result['prompt_type']})\n"
            report += f"**Overall Score**: {result.get('overall_score', 0):.2f}/5.0\n"
            report += f"**Agent Response**: {result.get('agent_response', '')[:150]}...\n"
            
            if "behavioral_analysis" in result:
                ba = result["behavioral_analysis"]
                report += f"**Behaviors Demonstrated**: {len(ba.get('behaviors_demonstrated', []))}/{len(result.get('expected_behaviors', []))}\n"
        
        if results["prompt_recommendations"]:
            report += "\n## Improvement Recommendations\n"
            for rec in results["prompt_recommendations"]:
                report += f"- **{rec['type'].replace('_', ' ').title()}**: {rec['recommendation']}\n"
        
        return report

# Example usage and testing
if __name__ == "__main__":
    def mock_agent(prompt):
        # Mock agent responses for different prompt types
        if "code review" in prompt.lower():
            return "I would carefully review the code for potential bugs, security issues, and performance problems. I would provide specific feedback with examples and suggest improvements while maintaining a professional tone."
        elif "customer service" in prompt.lower():
            return "Hello! I'm here to help you with any technical issues or questions you may have. Let me ask a few clarifying questions to better understand your situation and provide the most helpful solution."
        elif "data analysis" in prompt.lower():
            return "I'll analyze this data using appropriate statistical methods. First, let me examine the data quality and identify any potential issues. Then I'll apply relevant techniques to extract meaningful insights."
        elif "einstein" in prompt.lower():
            return "Ah, yes, this is very interesting. In my experience with theoretical physics, we must always question our assumptions and look at problems from multiple angles. Tell me, what specific aspect would you like to explore?"
        else:
            return "I'll help you with this task by following a systematic approach and providing clear, actionable guidance."
    
    # Run evaluation
    evaluator = SystemPromptEvaluator()
    results = evaluator.evaluate_system_prompt(mock_agent, scenario_ids=["instruction_001", "conversation_001", "role_001"])
    
    # Generate and print report
    report = evaluator.generate_comprehensive_report(results)
    logging.info(report)
