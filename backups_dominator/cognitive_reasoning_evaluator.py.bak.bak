#!/usr/bin/env python3
"""
Cognitive Reasoning Evaluator for AI Agents
Comprehensive tests for logical reasoning, problem-solving, and deduction capabilities
"""

import json
import time
import uuid
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import re

class ReasoningType(Enum):
    DEDUCTIVE = "deductive"
    INDUCTIVE = "inductive"
    ABDUCTIVE = "abductive"
    ANALOGICAL = "analogical"
    CAUSAL = "causal"
    TEMPORAL = "temporal"
    SPATIAL = "spatial"
    NUMERICAL = "numerical"
    SYLLOGISTIC = "syllogistic"

@dataclass
class CognitiveTestCase:
    """Individual cognitive reasoning test case"""
    id: str
    reasoning_type: ReasoningType
    question: str
    context: str
    expected_answer: str
    reasoning_steps: List[str]
    difficulty_level: int  # 1-5 scale
    domain: str  # math, logic, everyday, abstract
    scoring_criteria: Dict[str, float]
    test_cases: List[Dict[str, Any]]

class CognitiveReasoningEvaluator:
    """Main cognitive reasoning evaluation engine"""
    
    def __init__(self):
        self.test_suite = self._initialize_test_suite()
        self.evaluation_results = []
        
    def _initialize_test_suite(self) -> List[CognitiveTestCase]:
        """Initialize comprehensive cognitive reasoning test cases"""
        return [
            # DEDUCTIVE REASONING TESTS
            CognitiveTestCase(
                id="deductive_001",
                reasoning_type=ReasoningType.DEDUCTIVE,
                question="All mammals are warm-blooded. Whales are mammals. Therefore:",
                context="Logic syllogism test",
                expected_answer="Whales are warm-blooded",
                reasoning_steps=[
                    "Identify major premise: All mammals are warm-blooded",
                    "Identify minor premise: Whales are mammals", 
                    "Apply modus ponens: If A→B and A, then B",
                    "Conclusion: Whales are warm-blooded"
                ],
                difficulty_level=2,
                domain="logic",
                scoring_criteria={
                    "correct_conclusion": 0.4,
                    "valid_reasoning_steps": 0.3,
                    "logical_structure": 0.3
                },
                test_cases=[]
            ),
            
            CognitiveTestCase(
                id="deductive_002", 
                reasoning_type=ReasoningType.DEDUCTIVE,
                question="If it rains, the ground gets wet. The ground is wet. Therefore:",
                context="Affirming the consequent fallacy test",
                expected_answer="We cannot conclude it rained (affirming the consequent is a logical fallacy)",
                reasoning_steps=[
                    "Identify premise: If P then Q (rain → wet ground)",
                    "Recognize: Q is true (ground is wet)",
                    "Note: This does not prove P is true",
                    "Conclusion: Cannot determine if it rained"
                ],
                difficulty_level=4,
                domain="logic",
                scoring_criteria={
                    "fallacy_awareness": 0.5,
                    "correct_conclusion": 0.3,
                    "explanation_quality": 0.2
                },
                test_cases=[]
            ),
            
            # INDUCTIVE REASONING TESTS  
            CognitiveTestCase(
                id="inductive_001",
                reasoning_type=ReasoningType.INDUCTIVE,
                question="I've seen 10 white swans and no black swans. What can we conclude?",
                context="Inductive generalization test",
                expected_answer="All swans are likely white, but we cannot be certain (future black swans possible)",
                reasoning_steps=[
                    "Observe pattern: 10 white swans observed",
                    "Note absence of counter-examples: No black swans seen",
                    "Apply inductive generalization",
                    "Qualify conclusion with uncertainty"
                ],
                difficulty_level=3,
                domain="logic",
                scoring_criteria={
                    "pattern_recognition": 0.3,
                    "appropriate_qualification": 0.4,
                    "counterfactual_thinking": 0.3
                },
                test_cases=[]
            ),
            
            # ABDUCTIVE REASONING TESTS
            CognitiveTestCase(
                id="abductive_001",
                reasoning_type=ReasoningType.ABDUCTIVE,
                question="The grass is wet. What is the most likely explanation?",
                context="Abductive reasoning test",
                expected_answer="It rained recently (most probable explanation among common causes)",
                reasoning_steps=[
                    "Observe effect: Wet grass",
                    "Generate possible causes: rain, sprinkler, dew, spilled water",
                    "Evaluate likelihood: Rain is most probable given context",
                    "Select best explanation: Most likely cause"
                ],
                difficulty_level=2,
                domain="everyday",
                scoring_criteria={
                    "hypothesis_generation": 0.3,
                    "likelihood_assessment": 0.4,
                    "best_explanation_choice": 0.3
                },
                test_cases=[]
            ),
            
            # ANALOGICAL REASONING TESTS
            CognitiveTestCase(
                id="analogical_001", 
                reasoning_type=ReasoningType.ANALOGICAL,
                question="As the heart is to the body, what is to an organization?",
                context="Analogical reasoning test",
                expected_answer="Leadership/management (heart pumps blood to body parts, leadership guides organization)",
                reasoning_steps=[
                    "Identify source analogy: Heart → Body relationship",
                    "Analyze relationship: Heart pumps blood, sustains life, coordinates body",
                    "Identify target domain: Organization",
                    "Find analogous element: Leadership manages, guides, sustains organization"
                ],
                difficulty_level=3,
                domain="abstract",
                scoring_criteria={
                    "analogy_understanding": 0.4,
                    "relationship_mapping": 0.3,
                    "creative_mapping": 0.3
                },
                test_cases=[]
            ),
            
            # CAUSAL REASONING TESTS
            CognitiveTestCase(
                id="causal_001",
                reasoning_type=ReasoningType.CAUSAL, 
                question="Event A happened, then Event B. Did A cause B?",
                context="Causation vs correlation test",
                expected_answer="Cannot conclude causation from temporal sequence alone",
                reasoning_steps=[
                    "Observe temporal order: A then B",
                    "Note: Temporal order doesn't prove causation",
                    "Consider alternative explanations: coincidence, common cause, reverse causation",
                    "Require additional evidence for causal claim"
                ],
                difficulty_level=4,
                domain="logic",
                scoring_criteria={
                    "temporal_reasoning": 0.2,
                    "causal_logic_understanding": 0.5,
                    "alternative_explanations": 0.3
                },
                test_cases=[]
            ),
            
            # TEMPORAL REASONING TESTS
            CognitiveTestCase(
                id="temporal_001",
                reasoning_type=ReasoningType.TEMPORAL,
                question="If Sarah arrives before Tom, and Tom arrives after Mary, what can we say about Sarah and Mary?",
                context="Temporal ordering test",
                expected_answer="Sarah could arrive before, after, or at the same time as Mary",
                reasoning_steps=[
                    "Sequence 1: Sarah before Tom",
                    "Sequence 2: Tom after Mary (Mary before Tom)",
                    "Combine: Sarah before Tom, Mary before Tom",
                    "Relative order: Sarah and Mary's order is undetermined"
                ],
                difficulty_level=3,
                domain="logic",
                scoring_criteria={
                    "sequence_parsing": 0.4,
                    "transitive_reasoning": 0.3,
                    "indeterminacy_recognition": 0.3
                },
                test_cases=[]
            ),
            
            # SPATIAL REASONING TESTS
            CognitiveTestCase(
                id="spatial_001",
                reasoning_type=ReasoningType.SPATIAL,
                question="If you're facing north and turn left twice, which direction are you facing?",
                context="Spatial rotation test", 
                expected_answer="South",
                reasoning_steps=[
                    "Start orientation: Facing north",
                    "First left turn: Face west", 
                    "Second left turn: Face south",
                    "Final orientation: South"
                ],
                difficulty_level=2,
                domain="spatial",
                scoring_criteria={
                    "spatial_transformation": 0.5,
                    "orientation_tracking": 0.3,
                    "calculation_accuracy": 0.2
                },
                test_cases=[]
            ),
            
            # NUMERICAL REASONING TESTS
            CognitiveTestCase(
                id="numerical_001",
                reasoning_type=ReasoningType.NUMERICAL,
                question="If 2x + 5 = 15, what is x?",
                context="Algebraic reasoning test",
                expected_answer="x = 5",
                reasoning_steps=[
                    "Start equation: 2x + 5 = 15",
                    "Subtract 5: 2x = 10", 
                    "Divide by 2: x = 5",
                    "Verify: 2(5) + 5 = 15 ✓"
                ],
                difficulty_level=2,
                domain="math",
                scoring_criteria={
                    "correct_solution": 0.5,
                    "step_by_step_process": 0.3,
                    "verification": 0.2
                },
                test_cases=[]
            ),
            
            # SYLLOGISTIC REASONING TESTS
            CognitiveTestCase(
                id="syllogistic_001",
                reasoning_type=ReasoningType.SYLLOGISTIC,
                question="All A are B. Some B are C. What can we conclude?",
                context="Categorical syllogism test",
                expected_answer="We cannot conclude anything definite about A and C relationship",
                reasoning_steps=[
                    "Major premise: All A are B (universal affirmative)",
                    "Minor premise: Some B are C (particular affirmative)",
                    "Apply syllogistic rules",
                    "Conclusion: No valid conclusion about A and C"
                ],
                difficulty_level=5,
                domain="logic", 
                scoring_criteria={
                    "syllogistic_knowledge": 0.4,
                    "premise_analysis": 0.3,
                    "valid_conclusion_recognition": 0.3
                },
                test_cases=[]
            )
        ]
    
    def evaluate_agent(self, agent_function, test_subset: List[str] = None) -> Dict[str, Any]:
        """
        Evaluate an AI agent on cognitive reasoning tasks
        
        Args:
            agent_function: Function that takes a prompt and returns a response
            test_subset: Optional list of test IDs to run (default: all tests)
            
        Returns:
            Comprehensive evaluation results
        """
        if test_subset:
            tests_to_run = [t for t in self.test_suite if t.id in test_subset]
        else:
            tests_to_run = self.test_suite
            
        results = {
            "evaluation_id": str(uuid.uuid4()),
            "timestamp": time.time(),
            "total_tests": len(tests_to_run),
            "tests_completed": 0,
            "tests_passed": 0,
            "detailed_results": [],
            "summary_scores": {},
            "reasoning_type_scores": {},
            "domain_scores": {},
            "difficulty_analysis": {},
            "overall_score": 0.0
        }
        
        for test_case in tests_to_run:
            result = self._run_single_test(agent_function, test_case)
            results["detailed_results"].append(result)
            results["tests_completed"] += 1
            
            if result["passed"]:
                results["tests_passed"] += 1
                
        # Calculate aggregate scores
        self._calculate_aggregate_scores(results)
        
        return results
    
    def _run_single_test(self, agent_function, test_case: CognitiveTestCase) -> Dict[str, Any]:
        """Run a single cognitive reasoning test"""
        prompt = self._build_test_prompt(test_case)
        
        start_time = time.time()
        try:
            agent_response = agent_function(prompt)
            response_time = time.time() - start_time
            
            evaluation = self._evaluate_response(agent_response, test_case)
            
            return {
                "test_id": test_case.id,
                "reasoning_type": test_case.reasoning_type.value,
                "domain": test_case.domain,
                "difficulty": test_case.difficulty_level,
                "passed": evaluation["overall_score"] >= 0.7,
                "overall_score": evaluation["overall_score"],
                "detailed_scores": evaluation["component_scores"],
                "agent_response": agent_response,
                "expected_answer": test_case.expected_answer,
                "reasoning_steps": test_case.reasoning_steps,
                "response_time": response_time,
                "feedback": evaluation["feedback"]
            }
            
        except Exception as e:
            return {
                "test_id": test_case.id,
                "reasoning_type": test_case.reasoning_type.value,
                "domain": test_case.domain,
                "difficulty": test_case.difficulty_level,
                "passed": False,
                "overall_score": 0.0,
                "error": str(e),
                "response_time": time.time() - start_time
            }
    
    def _build_test_prompt(self, test_case: CognitiveTestCase) -> str:
        """Build test prompt for agent"""
        prompt = f"""
        COGNITIVE REASONING EVALUATION TEST
        
        Test Type: {test_case.reasoning_type.value.title()} Reasoning
        Domain: {test_case.domain.title()}
        Difficulty Level: {test_case.difficulty_level}/5
        
        Context: {test_case.context}
        
        Question: {test_case.question}
        
        Please provide:
        1. Your final answer/conclusion
        2. Step-by-step reasoning process
        3. Explanation of your logical process
        
        Focus on demonstrating clear, logical reasoning. Show your work and explain your logic.
        """
        return prompt
    
    def _evaluate_response(self, response: str, test_case: CognitiveTestCase) -> Dict[str, Any]:
        """Evaluate agent response against test case criteria"""
        scores = {}
        feedback = []
        
        # Evaluate based on specific criteria for this reasoning type
        if test_case.reasoning_type == ReasoningType.DEDUCTIVE:
            scores = self._evaluate_deductive_response(response, test_case)
        elif test_case.reasoning_type == ReasoningType.INDUCTIVE:
            scores = self._evaluate_inductive_response(response, test_case)
        elif test_case.reasoning_type == ReasoningType.ABDUCTIVE:
            scores = self._evaluate_abductive_response(response, test_case)
        elif test_case.reasoning_type == ReasoningType.ANALOGICAL:
            scores = self._evaluate_analogical_response(response, test_case)
        else:
            scores = self._evaluate_generic_response(response, test_case)
        
        # Calculate weighted overall score
        weighted_score = sum(
            scores.get(criterion, 0.0) * weight 
            for criterion, weight in test_case.scoring_criteria.items()
        )
        
        return {
            "component_scores": scores,
            "overall_score": weighted_score,
            "feedback": feedback
        }
    
    def _evaluate_deductive_response(self, response: str, test_case: CognitiveTestCase) -> Dict[str, float]:
        """Evaluate deductive reasoning response"""
        scores = {}
        
        # Check for correct conclusion
        if test_case.expected_answer.lower() in response.lower():
            scores["correct_conclusion"] = 1.0
        else:
            # Partial credit for related concepts
            if any(word in response.lower() for word in ["therefore", "thus", "so", "hence"]):
                scores["correct_conclusion"] = 0.3
            else:
                scores["correct_conclusion"] = 0.0
        
        # Check for valid reasoning steps
        reasoning_indicators = ["if", "then", "all", "some", "because", "since"]
        if any(indicator in response.lower() for indicator in reasoning_indicators):
            scores["valid_reasoning_steps"] = 1.0
        else:
            scores["valid_reasoning_steps"] = 0.0
            
        # Check for logical structure
        if "→" in response or "therefore" in response.lower():
            scores["logical_structure"] = 1.0
        else:
            scores["logical_structure"] = 0.5
            
        return scores
    
    def _evaluate_inductive_response(self, response: str, test_case: CognitiveTestCase) -> Dict[str, float]:
        """Evaluate inductive reasoning response"""
        scores = {}
        
        # Pattern recognition
        if "pattern" in response.lower() or "observed" in response.lower():
            scores["pattern_recognition"] = 1.0
        else:
            scores["pattern_recognition"] = 0.0
            
        # Appropriate qualification
        if "likely" in response.lower() or "probably" in response.lower() or "cannot be certain" in response.lower():
            scores["appropriate_qualification"] = 1.0
        else:
            scores["appropriate_qualification"] = 0.0
            
        # Counterfactual thinking
        if "possible" in response.lower() or "alternative" in response.lower():
            scores["counterfactual_thinking"] = 1.0
        else:
            scores["counterfactual_thinking"] = 0.0
            
        return scores
    
    def _evaluate_abductive_response(self, response: str, test_case: CognitiveTestCase) -> Dict[str, float]:
        """Evaluate abductive reasoning response"""
        scores = {}
        
        # Hypothesis generation
        response_lower = response.lower()
        if "because" in response_lower or "likely" in response_lower or "probably" in response_lower:
            scores["hypothesis_generation"] = 1.0
        else:
            scores["hypothesis_generation"] = 0.0
            
        # Likelihood assessment
        if "most likely" in response_lower or "best explanation" in response_lower:
            scores["likelihood_assessment"] = 1.0
        else:
            scores["likelihood_assessment"] = 0.5
            
        # Best explanation choice
        if len(response.split()) > 10:  # Detailed explanation
            scores["best_explanation_choice"] = 1.0
        else:
            scores["best_explanation_choice"] = 0.5
            
        return scores
    
    def _evaluate_analogical_response(self, response: str, test_case: CognitiveTestCase) -> Dict[str, float]:
        """Evaluate analogical reasoning response"""
        scores = {}
        
        # Analogy understanding
        response_lower = response.lower()
        if "as" in response_lower and "to" in response_lower:
            scores["analogy_understanding"] = 1.0
        else:
            scores["analogy_understanding"] = 0.0
            
        # Relationship mapping
        if "relationship" in response_lower or "like" in response_lower or "similar" in response_lower:
            scores["relationship_mapping"] = 1.0
        else:
            scores["relationship_mapping"] = 0.0
            
        # Creative mapping (partial credit for any meaningful mapping)
        if len(response.split()) > 5:
            scores["creative_mapping"] = 1.0
        else:
            scores["creative_mapping"] = 0.5
            
        return scores
    
    def _evaluate_generic_response(self, response: str, test_case: CognitiveTestCase) -> Dict[str, float]:
        """Generic evaluation for other reasoning types"""
        scores = {}
        
        # Simple heuristics based on response length and presence of key concepts
        response_lower = response.lower()
        
        for criterion in test_case.scoring_criteria.keys():
            if criterion == "correct_solution" and test_case.domain == "math":
                # Try to extract numerical answer
                numbers = re.findall(r'\d+', response)
                expected_numbers = re.findall(r'\d+', test_case.expected_answer)
                if numbers and expected_numbers and numbers[0] == expected_numbers[0]:
                    scores[criterion] = 1.0
                else:
                    scores[criterion] = 0.0
            else:
                # Default scoring based on response quality
                if len(response.split()) > 10:  # Detailed response
                    scores[criterion] = 0.8
                elif len(response.split()) > 3:  # Brief response
                    scores[criterion] = 0.5
                else:  # Minimal response
                    scores[criterion] = 0.2
                    
        return scores
    
    def _calculate_aggregate_scores(self, results: Dict[str, Any]) -> None:
        """Calculate aggregate scores and statistics"""
        detailed_results = results["detailed_results"]
        
        # Overall score
        total_score = sum(r["overall_score"] for r in detailed_results)
        results["overall_score"] = total_score / len(detailed_results) if detailed_results else 0.0
        
        # Reasoning type scores
        reasoning_type_groups = {}
        for result in detailed_results:
            reasoning_type = result["reasoning_type"]
            if reasoning_type not in reasoning_type_groups:
                reasoning_type_groups[reasoning_type] = []
            reasoning_type_groups[reasoning_type].append(result["overall_score"])
            
        results["reasoning_type_scores"] = {
            rtype: sum(scores) / len(scores) 
            for rtype, scores in reasoning_type_groups.items()
        }
        
        # Domain scores
        domain_groups = {}
        for result in detailed_results:
            domain = result["domain"]
            if domain not in domain_groups:
                domain_groups[domain] = []
            domain_groups[domain].append(result["overall_score"])
            
        results["domain_scores"] = {
            domain: sum(scores) / len(scores)
            for domain, scores in domain_groups.items()
        }
        
        # Difficulty analysis
        difficulty_groups = {}
        for result in detailed_results:
            difficulty = result["difficulty"]
            if difficulty not in difficulty_groups:
                difficulty_groups[difficulty] = []
            difficulty_groups[difficulty].append(result["overall_score"])
            
        results["difficulty_analysis"] = {
            f"level_{difficulty}": {
                "average_score": sum(scores) / len(scores),
                "test_count": len(scores)
            }
            for difficulty, scores in difficulty_groups.items()
        }
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """Generate comprehensive evaluation report"""
        report = f"""
# COGNITIVE REASONING EVALUATION REPORT

## Summary Statistics
- **Overall Score**: {results['overall_score']:.2f}/1.00 ({results['overall_score']*100:.1f}%)
- **Tests Passed**: {results['tests_passed']}/{results['tests_completed']} ({results['tests_passed']/results['tests_completed']*100:.1f}%)
- **Evaluation ID**: {results['evaluation_id']}

## Reasoning Type Performance
"""
        
        for reasoning_type, score in results["reasoning_type_scores"].items():
            report += f"- **{reasoning_type.title()}**: {score:.2f}/1.00 ({score*100:.1f}%)\n"
            
        report += "\n## Domain Performance\n"
        for domain, score in results["domain_scores"].items():
            report += f"- **{domain.title()}**: {score:.2f}/1.00 ({score*100:.1f}%)\n"
            
        report += "\n## Difficulty Analysis\n"
        for level, stats in results["difficulty_analysis"].items():
            report += f"- **{level.replace('_', ' ').title()}**: {stats['average_score']:.2f}/1.00 ({stats['average_score']*100:.1f}%) - {stats['test_count']} tests\n"
            
        report += "\n## Detailed Test Results\n"
        for result in results["detailed_results"]:
            status = "✅ PASS" if result["passed"] else "❌ FAIL"
            report += f"\n### {result['test_id']} - {result['reasoning_type'].title()} ({result['domain']})\n"
            report += f"**Status**: {status} | **Score**: {result['overall_score']:.2f}/1.00\n"
            report += f"**Agent Response**: {result['agent_response'][:200]}...\n"
            report += f"**Expected**: {result['expected_answer']}\n"
            
        return report

# Example usage and testing
if __name__ == "__main__":
    # Mock agent function for testing
    def mock_agent(prompt):
        # Simple rule-based responses for demonstration
        if "deductive" in prompt.lower():
            return "Therefore, the conclusion follows logically from the premises using deductive reasoning."
        elif "inductive" in prompt.lower():
            return "Based on the pattern observed, it is likely that this generalization holds, though we cannot be completely certain."
        elif "abductive" in prompt.lower():
            return "The most likely explanation is that this caused it, based on the available evidence."
        else:
            return "This is a reasoned response based on logical analysis of the given information."
    
    # Run evaluation
    evaluator = CognitiveReasoningEvaluator()
    results = evaluator.evaluate_agent(mock_agent, test_subset=["deductive_001", "inductive_001", "abductive_001"])
    
    # Generate and print report
    report = evaluator.generate_report(results)
    logging.info(report)
