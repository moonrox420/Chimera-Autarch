# ðŸ§  Qwen 2.5 Coder Integration Guide

**Using Qwen 2.5 Coder 14B Instruct (Abliterated) with CHIMERA AUTARCH**

## Overview

Qwen 2.5 Coder is a state-of-the-art coding model that excels at:
- âœ… **Code generation** - Writes clean, idiomatic Python
- âœ… **Test generation** - Creates comprehensive pytest suites
- âœ… **Code analysis** - Understands complex codebases
- âœ… **Bug fixing** - Suggests accurate patches
- âœ… **Uncensored** - No safety restrictions on code generation
- âœ… **14B parameters** - High quality output
- âœ… **Q5_K_M quantized** - Runs on consumer hardware

## Quick Start

### 1. Install Ollama

```bash
# Linux/macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Download from: https://ollama.com/download
```

### 2. Pull Qwen 2.5 Coder Model

```bash
# Pull the abliterated (uncensored) version
ollama pull dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m

# Verify installation
ollama list
```

Expected output:
```
NAME                                                    ID              SIZE      MODIFIED
dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m    abc123def456    9.2 GB    2 minutes ago
```

### 3. Configure CHIMERA

CHIMERA auto-detects Qwen 2.5 Coder as the preferred model:

```python
# No configuration needed! CHIMERA automatically uses Qwen 2.5 Coder
# if it's available in Ollama
```

### 4. Test the Integration

```bash
# Test code generation
python test_llm.py
```

Expected output:
```
ðŸ” Testing LLM Integration...
âœ… Using Local LLM provider (Ollama)
âœ… Model: dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m

ðŸ§ª Generating fibonacci function...
âœ… Generated 25 lines of code
âœ… Generated 18 lines of tests

ðŸ§ª Running generated tests...
âœ… All tests passed!
```

## Model Comparison

| Model | Size | Speed | Quality | Cost | Notes |
|-------|------|-------|---------|------|-------|
| **Qwen 2.5 Coder 14B** | 9.2 GB | Medium | â­â­â­â­â­ | Free | **BEST** - Highest quality, uncensored |
| DeepSeek Coder 6.7B | 4.1 GB | Fast | â­â­â­â­ | Free | Good balance of speed/quality |
| CodeLlama 7B | 3.8 GB | Fast | â­â­â­ | Free | Reliable, well-tested |
| GPT-4 | N/A | Fast | â­â­â­â­â­ | $0.03 | Best quality but costs money |
| Claude 3.5 | N/A | Fast | â­â­â­â­â­ | $0.015 | Excellent but costs money |

## Hardware Requirements

### Minimum (Qwen 2.5 Coder Q5_K_M)
- **RAM**: 16 GB
- **VRAM**: 8 GB (GPU) or 0 GB (CPU-only)
- **Storage**: 12 GB free
- **CPU**: 4+ cores

### Recommended
- **RAM**: 32 GB
- **VRAM**: 12 GB (GPU)
- **Storage**: 20 GB free
- **CPU**: 8+ cores

### GPU Acceleration (Optional)
```bash
# Check if Ollama is using GPU
ollama run dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m

# In another terminal
nvidia-smi  # Should show ollama process
```

## Advanced Configuration

### Custom Model Selection

```python
from llm_integration import LocalLLMProvider

# Use specific model
provider = LocalLLMProvider(
    base_url="http://localhost:11434",
    model="dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m"
)

# Or use auto-detection (default)
provider = LocalLLMProvider()  # Auto-selects Qwen 2.5 Coder if available
```

### Model Priority

CHIMERA tries models in this order:
1. `dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m` â­ **Best**
2. `qwen2.5-coder:14b` - Official Qwen 2.5
3. `deepseek-coder:6.7b` - DeepSeek Coder
4. `codellama:7b-code` - CodeLlama code-specific
5. `codellama` - CodeLlama default

### Fallback Behavior

If Qwen 2.5 Coder fails, CHIMERA automatically falls back to CodeLlama:

```python
# Automatic fallback in llm_integration.py
try:
    result = await generate_code_with_qwen()
except Exception as e:
    logger.info("Qwen failed, using CodeLlama fallback...")
    result = await generate_code_with_codellama()
```

## Usage Examples

### Example 1: Generate Function

```python
from llm_integration import CodeGenerator

generator = CodeGenerator()

patch = await generator.generate_patch(
    topic="file_processing",
    failure_reason="Need async file reader with error handling",
    context={
        "function_name": "read_file_async",
        "parameters": ["file_path: str"],
        "return_type": "Dict[str, Any]"
    },
    goal="Create robust async file reader"
)

logging.info(f"Generated code:\n{patch.code}")
```

Output (from Qwen 2.5 Coder):
```python
import asyncio
import aiofiles
from pathlib import Path
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

async def read_file_async(file_path: str) -> Dict[str, Any]:
    """Read file asynchronously with comprehensive error handling"""
    try:
        path = Path(file_path)
        
        if not path.exists():
            logger.error(f"File not found: {file_path}")
            return {"success": False, "error": "File not found"}
        
        if not path.is_file():
            logger.error(f"Not a file: {file_path}")
            return {"success": False, "error": "Not a file"}
        
        async with aiofiles.open(file_path, mode='r', encoding='utf-8') as f:
            content = await f.read()
            logger.info(f"Successfully read {len(content)} bytes from {file_path}")
            return {
                "success": True,
                "content": content,
                "size": len(content),
                "path": str(path.absolute())
            }
    
    except PermissionError as e:
        logger.error(f"Permission denied: {file_path}")
        return {"success": False, "error": "Permission denied"}
    except UnicodeDecodeError as e:
        logger.error(f"Encoding error in {file_path}: {e}")
        return {"success": False, "error": "Encoding error"}
    except Exception as e:
        logger.error(f"Unexpected error reading {file_path}: {e}")
        return {"success": False, "error": str(e)}
```

### Example 2: Generate Tests

```python
# CHIMERA automatically generates tests for the code above
tests = await generator._generate_test_code(
    patch.code,
    {"function_name": "read_file_async"}
)

logging.info(f"Generated tests:\n{tests}")
```

Output (from Qwen 2.5 Coder):
```python
import pytest
import asyncio
from pathlib import Path
from unittest.mock import AsyncMock, MagicMock, patch
import aiofiles

@pytest.mark.asyncio
async def test_read_file_async_success(tmp_path):
    """Test successful file reading"""
    test_file = tmp_path / "test.txt"
    test_content = "Hello, CHIMERA!"
    test_file.write_text(test_content)
    
    result = await read_file_async(str(test_file))
    
    assert result["success"] is True
    assert result["content"] == test_content
    assert result["size"] == len(test_content)
    assert Path(result["path"]) == test_file.absolute()

@pytest.mark.asyncio
async def test_read_file_async_not_found():
    """Test file not found error"""
    result = await read_file_async("/nonexistent/file.txt")
    
    assert result["success"] is False
    assert "not found" in result["error"].lower()

@pytest.mark.asyncio
async def test_read_file_async_not_a_file(tmp_path):
    """Test directory instead of file error"""
    test_dir = tmp_path / "test_dir"
    test_dir.mkdir()
    
    result = await read_file_async(str(test_dir))
    
    assert result["success"] is False
    assert "not a file" in result["error"].lower()
```

### Example 3: Fix Bug

```python
# CHIMERA detected a bug and asks Qwen 2.5 Coder to fix it
patch = await generator.generate_patch(
    topic="networking",
    failure_reason="Connection timeout not handled properly",
    context={
        "existing_code": """
async def fetch_data(url: str):
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        return response.json()
""",
        "error": "httpx.TimeoutException not caught"
    },
    goal="Add timeout handling with retry logic"
)
```

Output (from Qwen 2.5 Coder):
```python
import asyncio
import httpx
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

async def fetch_data(
    url: str,
    timeout: float = 10.0,
    max_retries: int = 3,
    retry_delay: float = 1.0
) -> Optional[Dict[str, Any]]:
    """
    Fetch data from URL with timeout and retry logic
    
    Args:
        url: URL to fetch
        timeout: Request timeout in seconds
        max_retries: Maximum number of retry attempts
        retry_delay: Delay between retries in seconds
    
    Returns:
        JSON response or None on failure
    """
    for attempt in range(max_retries):
        try:
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.get(url)
                response.raise_for_status()
                logger.info(f"Successfully fetched data from {url}")
                return response.json()
        
        except httpx.TimeoutException:
            logger.warning(f"Timeout fetching {url} (attempt {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                await asyncio.sleep(retry_delay)
            else:
                logger.error(f"Max retries reached for {url}")
                return None
        
        except httpx.HTTPStatusError as e:
            logger.error(f"HTTP error {e.response.status_code} from {url}")
            return None
        
        except Exception as e:
            logger.error(f"Unexpected error fetching {url}: {e}")
            return None
    
    return None
```

## Performance Tuning

### Increase Context Window

```python
# In llm_integration.py, modify LocalLLMProvider.generate_code()
response = await self.client.post(
    f"{self.base_url}/api/generate",
    json={
        "model": self.model,
        "prompt": full_prompt,
        "stream": False,
        "options": {
            "temperature": 0.2,
            "top_p": 0.9,
            "num_predict": 4000,  # Increase from 2000
            "num_ctx": 8192,      # Increase context window
        }
    }
)
```

### Adjust Temperature

```python
# Lower temperature = more deterministic
"temperature": 0.1  # Very focused, consistent

# Higher temperature = more creative
"temperature": 0.4  # More variety, less consistent
```

### GPU Acceleration

```bash
# Check GPU usage
watch -n 1 nvidia-smi

# Set Ollama to use specific GPU
CUDA_VISIBLE_DEVICES=0 ollama serve
```

## Troubleshooting

### Issue: "Model not found"
```bash
# Re-pull the model
ollama pull dagbs/qwen2.5-coder-14b-instruct-abliterated:q5_k_m

# Verify
ollama list
```

### Issue: "Out of memory"
```bash
# Use smaller quantization
ollama pull qwen2.5-coder:7b  # Smaller 7B model

# Or use Q4 quantization (lower quality, less memory)
ollama pull dagbs/qwen2.5-coder-14b-instruct-abliterated:q4_k_m
```

### Issue: "Generation too slow"
```bash
# Use GPU acceleration (if available)
# Or switch to faster model:
ollama pull deepseek-coder:6.7b
```

### Issue: "Code has markdown formatting"
CHIMERA automatically strips markdown:
```python
# Automatic cleanup in llm_integration.py
if "```python" in generated:
    code = generated.split("```python")[1].split("```")[0].strip()
```

## Best Practices

1. **Use Qwen 2.5 Coder for complex tasks** - Function generation, refactoring
2. **Use DeepSeek Coder for speed** - Simple tasks, rapid iteration
3. **Monitor memory usage** - Close other applications if needed
4. **Keep context concise** - Better results with focused prompts
5. **Review generated code** - Always verify before applying
6. **Run tests** - CHIMERA auto-generates tests for verification

## Model Updates

Check for new versions:
```bash
# List available versions
ollama search qwen2.5-coder

# Pull latest
ollama pull dagbs/qwen2.5-coder-14b-instruct-abliterated:latest
```

---

**Questions?** Check `llm_integration.py` for implementation details or run `python test_llm.py` to verify your setup.
